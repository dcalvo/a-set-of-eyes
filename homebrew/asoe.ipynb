{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dog_captioning",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.6 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "eea840d32bf56a928ee27a035e52aac990f88e7d3a2b564ede6a6b77b7629b2f"
        }
      }
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eMn6eFflbKo",
        "outputId": "b053eb14-5450-4ece-d5ad-034b49ae051a"
      },
      "source": [
        "# tensorflow version\n",
        "import tensorflow\n",
        "print('tensorflow: %s' % tensorflow.__version__)\n",
        "# keras version\n",
        "import tensorflow.keras\n",
        "print('keras: %s' % tensorflow.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ilcKFF19MbO"
      },
      "source": [
        "# We create the CV model and extract features from our images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YWJ9Ofulo52",
        "outputId": "9f5ebdf9-acce-41b2-9f73-821418d23506",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "\n",
        "# extract features from each photo in the directory\n",
        "def extract_features(directory):\n",
        "\t# load the model without predicitions layer\n",
        "\tmodel = EfficientNetB7(include_top=False, input_shape=(600, 600, 3), pooling=\"max\")\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\t# extract features from each photo\n",
        "\tfeatures = dict()\n",
        "\tfor name in listdir(directory):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timage = load_img(filename, target_size=(600, 600))\n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# reshape data for the model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# prepare the image for the EfficientNet model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get features\n",
        "\t\tfeature = model.predict(image, verbose=2)\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\t# store feature\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\tprint('>%s' % name)\n",
        "\treturn features\n",
        "\n",
        "# extract featflickr/Flicker8k_Datasetges\n",
        "directory = 'png'\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "dump(features, open('webpagegen_10k_b7_features.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# We extract captions for the images and create a cleaned vocabulary + answer key"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "O4Y5ITb-9V8T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dji_-W-c1DUD",
        "outputId": "22213235-d8e5-43ed-faf6-86c95417da1f"
      },
      "source": [
        "import string\n",
        "from os import listdir\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# transform gui dir into descriptions doc\n",
        "def create_descriptions(gui_dir):\n",
        "    descriptions = []\n",
        "    gui_filenames = listdir(gui_dir)\n",
        "\n",
        "    for gui_filename in gui_filenames:\n",
        "        gui_id = gui_filename.split('.')[0]\n",
        "        gui_contents = load_doc(gui_dir + \"/\" + gui_filename)\n",
        "\t\t# TEMPORARY FIX FOR DATASET MALFORMAT\n",
        "        gui_contents = gui_contents.replace(\"body{\", \"body {\")\n",
        "        gui_contents = gui_contents.replace(\"table{\", \"table {\")\n",
        "\t\t# END TEMPORARY FIX\n",
        "        gui_contents = gui_contents.replace(\",\", \" ,\")\n",
        "        gui_contents = ' '.join(gui_contents.split())\n",
        "        descriptions.append(gui_id + ' ' + gui_contents)\n",
        "    \n",
        "    data = '\\n'.join(descriptions)\n",
        "    file = open('descriptions.txt', 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take the first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\t# store description\n",
        "\t\tmapping[image_id].append(image_desc)\n",
        "\treturn mapping\n",
        "\n",
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t# build a list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# transform gui dir to descriptions\n",
        "create_descriptions('gui')\n",
        "# verify descriptions\n",
        "filename = 'descriptions.txt'\n",
        "doc = load_doc(filename)\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "# summarize vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lm6qp8RrSw0"
      },
      "source": [
        "# Load the data and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRbPIvM-plvI",
        "outputId": "06997303-089c-41a3-fb8f-605c185def85"
      },
      "source": [
        "import tensorflow\n",
        "from numpy import array\n",
        "from pickle import load\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split()[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "    # load document\n",
        "    doc = load_doc(filename)\n",
        "    descriptions = dict()\n",
        "    for line in doc.split('\\n'):\n",
        "        # split line by white space\n",
        "        tokens = line.split()\n",
        "        # split id from description\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        # skip images not in the set\n",
        "        if image_id in dataset:\n",
        "            # create list\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = list()\n",
        "            # wrap description in tokens\n",
        "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "            # store\n",
        "            descriptions[image_id].append(desc)\n",
        "    return descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features\n",
        "\n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each image identifier\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# walk through each description for the image\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\t# encode the sequence\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t\t# split one sequence into multiple X,y pairs\n",
        "\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t# split into input and output pair\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t# pad input sequence\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t\t# encode output sequence\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t# store\n",
        "\t\t\t\tX1.append(photos[key][0])\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn array(X1), array(X2), array(y)\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "\t# feature extractor model\n",
        "\tinputs1 = Input(shape=(2560,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\t# decoder model\n",
        "\tdecoder1 = add([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# summarize model\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model\n",
        "\n",
        "# train dataset\n",
        "\n",
        "# load training dataset (8K)\n",
        "descriptions = 'descriptions.txt'\n",
        "features = 'webpagegen_10k_b7_features.pkl'\n",
        "dataset = load_set(descriptions)\n",
        "print('Dataset: %d' % len(dataset))\n",
        "\n",
        "# RANDOMLY split into train and test sets\n",
        "train = set(list(dataset)[:8000])\n",
        "test = set(list(dataset)[8000:])\n",
        "\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(descriptions, train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "test_descriptions = load_clean_descriptions(descriptions, test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "\n",
        "# photo features\n",
        "train_features = load_photo_features(features, train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "test_features = load_photo_features(features, test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# determine the maximum sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        "\n",
        "# prepare sequences\n",
        "print(\"Encoding training data...\")\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n",
        "print(\"Encoding testing data...\")\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)\n",
        "print(\"Encoding complete\")\n",
        "\n",
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# define checkpoint callback\n",
        "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit model\n",
        "print(\"Let the training commence!\")\n",
        "model.fit([X1train, X2train], ytrain, epochs=20, verbose=1, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
      ]
    },
    {
      "source": [
        "# Todo: Evaluate, Predict"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}